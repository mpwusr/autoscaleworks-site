<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Efficient LLM Deployment: vLLM + Ray Serve + KubeRay + Kubernetes</title>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/theme/black.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@400;500;600;700&display=swap">
<style>
  :root {
    --accent: #FF6B35;
    --accent2: #00D4AA;
    --accent3: #7C5CFC;
    --bg-dark: #0a0a0f;
    --bg-card: #12121a;
    --text-primary: #f0f0f5;
    --text-muted: #8a8a9a;
    --gradient-1: linear-gradient(135deg, #FF6B35, #FF3366);
    --gradient-2: linear-gradient(135deg, #00D4AA, #00A3FF);
    --gradient-3: linear-gradient(135deg, #7C5CFC, #FF6BCC);
  }

  .reveal {
    font-family: 'Inter', -apple-system, sans-serif;
    color: var(--text-primary);
  }

  .reveal .slides {
    text-align: left;
  }

  .reveal .slides section {
    padding: 40px 60px;
  }

  .reveal h1, .reveal h2, .reveal h3, .reveal h4 {
    font-family: 'Inter', sans-serif;
    font-weight: 800;
    text-transform: none;
    letter-spacing: -0.02em;
    line-height: 1.15;
  }

  .reveal h1 { font-size: 2.8em; }
  .reveal h2 { font-size: 2.0em; margin-bottom: 0.6em; }
  .reveal h3 { font-size: 1.4em; color: var(--accent); margin-bottom: 0.5em; }

  .reveal p, .reveal li {
    font-size: 0.75em;
    line-height: 1.7;
    color: var(--text-muted);
  }

  .reveal li {
    margin-bottom: 0.5em;
  }

  .reveal code, .reveal pre code {
    font-family: 'JetBrains Mono', monospace;
  }

  .reveal pre {
    width: 100%;
    box-shadow: none;
  }

  .reveal pre code {
    font-size: 0.65em;
    line-height: 1.8;
    padding: 25px 30px;
    border-radius: 12px;
    background: #1a1a2e;
    border: 1px solid rgba(255,255,255,0.08);
  }

  /* Gradient text utility */
  .grad-orange { background: var(--gradient-1); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
  .grad-teal { background: var(--gradient-2); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }
  .grad-purple { background: var(--gradient-3); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; }

  /* Tag / badge */
  .tag {
    display: inline-block;
    padding: 4px 14px;
    border-radius: 20px;
    font-size: 0.55em;
    font-weight: 600;
    letter-spacing: 0.04em;
    text-transform: uppercase;
    margin-bottom: 12px;
  }
  .tag-orange { background: rgba(255,107,53,0.15); color: #FF6B35; border: 1px solid rgba(255,107,53,0.3); }
  .tag-teal { background: rgba(0,212,170,0.15); color: #00D4AA; border: 1px solid rgba(0,212,170,0.3); }
  .tag-purple { background: rgba(124,92,252,0.15); color: #7C5CFC; border: 1px solid rgba(124,92,252,0.3); }

  /* Grid layouts */
  .grid-2 { display: grid; grid-template-columns: 1fr 1fr; gap: 30px; margin-top: 20px; }
  .grid-3 { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 25px; margin-top: 20px; }
  .grid-4 { display: grid; grid-template-columns: 1fr 1fr 1fr 1fr; gap: 20px; margin-top: 20px; }

  /* Cards */
  .card {
    background: var(--bg-card);
    border: 1px solid rgba(255,255,255,0.06);
    border-radius: 16px;
    padding: 28px;
    transition: border-color 0.3s ease;
  }
  .card:hover { border-color: rgba(255,255,255,0.15); }
  .card h4 {
    font-size: 0.85em;
    font-weight: 700;
    margin-bottom: 10px;
    color: var(--text-primary);
  }
  .card p, .card li {
    font-size: 0.6em;
    color: var(--text-muted);
  }

  /* Numbered card */
  .num {
    display: inline-flex;
    align-items: center;
    justify-content: center;
    width: 36px;
    height: 36px;
    border-radius: 10px;
    font-size: 0.7em;
    font-weight: 800;
    margin-bottom: 14px;
  }
  .num-orange { background: rgba(255,107,53,0.15); color: #FF6B35; }
  .num-teal { background: rgba(0,212,170,0.15); color: #00D4AA; }
  .num-purple { background: rgba(124,92,252,0.15); color: #7C5CFC; }

  /* Stack diagram */
  .stack {
    display: flex;
    flex-direction: column;
    gap: 8px;
    max-width: 650px;
    margin: 20px auto;
  }
  .stack-layer {
    text-align: center;
    padding: 18px 30px;
    border-radius: 12px;
    font-weight: 700;
    font-size: 0.8em;
    letter-spacing: 0.02em;
    position: relative;
  }
  .stack-layer small {
    display: block;
    font-weight: 400;
    font-size: 0.7em;
    color: rgba(255,255,255,0.6);
    margin-top: 4px;
  }
  .stack-arrow {
    text-align: center;
    font-size: 1.2em;
    color: var(--text-muted);
    line-height: 1;
  }

  /* Metric box */
  .metric {
    text-align: center;
    padding: 20px;
  }
  .metric .value {
    font-size: 2.2em;
    font-weight: 900;
    line-height: 1;
    margin-bottom: 6px;
  }
  .metric .label {
    font-size: 0.55em;
    color: var(--text-muted);
    text-transform: uppercase;
    letter-spacing: 0.08em;
    font-weight: 500;
  }

  /* Horizontal rule */
  .divider {
    border: none;
    border-top: 1px solid rgba(255,255,255,0.08);
    margin: 25px 0;
  }

  /* Title slide special */
  .title-slide {
    text-align: center !important;
    display: flex !important;
    flex-direction: column;
    align-items: center;
    justify-content: center;
  }
  .title-slide h1 {
    font-size: 3.2em;
    margin-bottom: 0.2em;
  }
  .title-slide .subtitle {
    font-size: 0.9em;
    color: var(--text-muted);
    font-weight: 400;
    margin-bottom: 40px;
  }
  .title-slide .author {
    font-size: 0.65em;
    color: var(--text-muted);
  }

  /* Flow diagram */
  .flow {
    display: flex;
    align-items: center;
    justify-content: center;
    gap: 12px;
    margin: 20px 0;
    flex-wrap: wrap;
  }
  .flow-box {
    background: var(--bg-card);
    border: 1px solid rgba(255,255,255,0.1);
    border-radius: 10px;
    padding: 14px 22px;
    font-size: 0.65em;
    font-weight: 600;
    text-align: center;
    min-width: 120px;
  }
  .flow-arrow {
    font-size: 1.2em;
    color: var(--accent);
  }

  /* Hardware chips */
  .chip-row {
    display: flex;
    gap: 12px;
    flex-wrap: wrap;
    margin-top: 15px;
  }
  .chip {
    padding: 8px 18px;
    border-radius: 8px;
    font-size: 0.6em;
    font-weight: 600;
    border: 1px solid rgba(255,255,255,0.1);
    background: var(--bg-card);
  }
  .chip-green { border-color: #76B900; color: #76B900; }
  .chip-red { border-color: #ED1C24; color: #ED1C24; }
  .chip-blue { border-color: #0071C5; color: #0071C5; }
  .chip-gold { border-color: #F9AB00; color: #F9AB00; }

  /* Comparison table */
  .compare-table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 15px;
    font-size: 0.6em;
  }
  .compare-table th {
    text-align: left;
    padding: 12px 16px;
    border-bottom: 2px solid rgba(255,255,255,0.1);
    color: var(--accent);
    font-weight: 700;
    font-size: 0.95em;
  }
  .compare-table td {
    padding: 10px 16px;
    border-bottom: 1px solid rgba(255,255,255,0.05);
    color: var(--text-muted);
  }
  .compare-table tr:hover td {
    color: var(--text-primary);
  }

  /* Footer */
  .slide-footer {
    position: absolute;
    bottom: 20px;
    right: 60px;
    font-size: 0.45em;
    color: rgba(255,255,255,0.2);
    font-weight: 500;
  }

  /* Highlight box */
  .highlight-box {
    background: linear-gradient(135deg, rgba(255,107,53,0.08), rgba(124,92,252,0.08));
    border: 1px solid rgba(255,107,53,0.2);
    border-radius: 14px;
    padding: 22px 28px;
    margin: 15px 0;
  }
  .highlight-box p {
    color: var(--text-primary) !important;
    font-size: 0.7em !important;
  }

  /* Override reveal backgrounds */
  .reveal .slide-background {
    background: var(--bg-dark);
  }

  .reveal .progress { color: var(--accent); height: 3px; }
  .reveal .controls { color: var(--accent); }
</style>
</head>
<body>
<div class="reveal">
<div class="slides">

<!-- ============================================ -->
<!-- SLIDE 1: TITLE -->
<!-- ============================================ -->
<section class="title-slide" data-background-color="#0a0a0f">
  <span class="tag tag-orange">Technical Deep Dive</span>
  <h1>Efficient <span class="grad-orange">LLM</span> Deployment</h1>
  <p class="subtitle">A Unified Approach with vLLM, Ray Serve, KubeRay & Kubernetes</p>
  <hr class="divider" style="width: 100px; border-color: rgba(255,107,53,0.4);">
  <p class="author">
    Based on CNCF presentation by <strong style="color: var(--text-primary);">Lily (Xiaoxuan) Liu</strong><br>
    AutoscaleWorks &mdash; Saddle River Consulting LLC
  </p>
</section>

<!-- ============================================ -->
<!-- SLIDE 2: THE PROBLEM -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-orange">Context</span>
  <h2>The <span class="grad-orange">LLM Infrastructure</span> Challenge</h2>
  <p style="max-width: 700px;">Deploying large language models in production is fundamentally different from training. Serving demands low latency, high throughput, and cost efficiency &mdash; all at massive scale.</p>

  <div class="grid-3">
    <div class="card">
      <div class="num num-orange">1</div>
      <h4>GPU Memory Constraints</h4>
      <p>Models like Llama 3.1 405B exceed single-node GPU memory, requiring distributed inference across multiple nodes</p>
    </div>
    <div class="card">
      <div class="num num-orange">2</div>
      <h4>Scaling Complexity</h4>
      <p>Autoscaling GPU workloads requires orchestration-aware infrastructure that understands model topology</p>
    </div>
    <div class="card">
      <div class="num num-orange">3</div>
      <h4>Operational Overhead</h4>
      <p>Managing model lifecycle, rolling updates, health checks, and multi-model serving without downtime</p>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 3: ARCHITECTURE STACK -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Architecture</span>
  <h2>End-to-End <span class="grad-teal">LLM Serving</span> Stack</h2>

  <div class="stack">
    <div class="stack-layer" style="background: linear-gradient(135deg, #FF6B35, #FF3366); color: white;">
      vLLM
      <small>LLM Inference Engine &mdash; PagedAttention, Continuous Batching, Tensor Parallelism</small>
    </div>
    <div class="stack-arrow">&darr;</div>
    <div class="stack-layer" style="background: linear-gradient(135deg, #00D4AA, #00A3FF); color: white;">
      Ray Serve
      <small>Model Deployment &amp; Scaling &mdash; Request Routing, Load Balancing, Lifecycle Management</small>
    </div>
    <div class="stack-arrow">&darr;</div>
    <div class="stack-layer" style="background: linear-gradient(135deg, #7C5CFC, #9B6BFF); color: white;">
      KubeRay
      <small>Kubernetes Operator for Ray Clusters &mdash; Automated Cluster Management</small>
    </div>
    <div class="stack-arrow">&darr;</div>
    <div class="stack-layer" style="background: rgba(255,255,255,0.08); border: 1px solid rgba(255,255,255,0.15); color: var(--text-primary);">
      Kubernetes
      <small>Container Orchestration &mdash; GPU Scheduling, Node Pools, Networking</small>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 4: vLLM DEEP DIVE -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-orange">Layer 1</span>
  <h2><span class="grad-orange">vLLM</span> &mdash; The Inference Engine</h2>
  <p>A high-throughput, memory-efficient serving engine for LLMs, purpose-built for production inference.</p>

  <div class="grid-2">
    <div>
      <div class="card" style="margin-bottom: 20px;">
        <h4>Core Innovations</h4>
        <ul>
          <li><strong style="color: var(--accent);">PagedAttention</strong> &mdash; Near-zero memory waste for KV cache via OS-inspired virtual memory paging</li>
          <li><strong style="color: var(--accent);">Continuous Batching</strong> &mdash; Dynamically adds/removes requests from running batch for max GPU utilization</li>
          <li><strong style="color: var(--accent);">Tensor Parallelism</strong> &mdash; Splits model layers across GPUs for distributed inference</li>
          <li><strong style="color: var(--accent);">Pipeline Parallelism</strong> &mdash; Distributes model stages across nodes for multi-node serving</li>
        </ul>
      </div>
    </div>
    <div>
      <div class="card">
        <h4>OpenAI-Compatible API</h4>
        <pre><code>from vllm import LLM

# Create an LLM with HuggingFace model
llm = LLM(
  model="meta-llama/Meta-Llama-3.1-8B"
)

# Generate text from prompts
prompts = [
  "Hello, my name is",
  "The capital of France is"
]
outputs = llm.generate(prompts)</code></pre>
      </div>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 5: BROAD MODEL SUPPORT -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-orange">vLLM</span>
  <h2>Broad <span class="grad-orange">Model Support</span></h2>
  <p>vLLM supports nearly all popular open-source LLMs and VLMs out of the box.</p>

  <div class="grid-2" style="margin-top: 25px;">
    <div>
      <h3 style="font-size: 1em; margin-bottom: 15px;">Large Language Models</h3>
      <div class="card" style="margin-bottom: 12px;">
        <h4>Meta Llama 3.1</h4>
        <p>Official launch partner &mdash; 8B, 70B, 405B parameter variants. Day-one optimized support.</p>
      </div>
      <div class="card" style="margin-bottom: 12px;">
        <h4>DeepSeek R1 / V3</h4>
        <p>MoE architecture with Wide-EP support for distributed expert parallelism across nodes.</p>
      </div>
      <div class="card">
        <h4>Qwen, Mistral, Gemma, Phi</h4>
        <p>First-class support for all major model families from HuggingFace Hub.</p>
      </div>
    </div>
    <div>
      <h3 style="font-size: 1em; margin-bottom: 15px;">Vision Language Models</h3>
      <div class="card" style="margin-bottom: 12px;">
        <h4>Pixtral 12B</h4>
        <p>Mistral's multimodal model &mdash; contributed natively by model creators to vLLM.</p>
      </div>
      <div class="card" style="margin-bottom: 12px;">
        <h4>Qwen2-VL</h4>
        <p>Vision-language model with dynamic resolution support, contributed by Alibaba.</p>
      </div>
      <div class="card">
        <h4>LLaVA, InternVL, Molmo</h4>
        <p>Growing ecosystem of multimodal models with optimized attention and image encoders.</p>
      </div>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 6: PyTorch NARROW WAIST -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Hardware</span>
  <h2>PyTorch as the <span class="grad-teal">Narrow Waist</span></h2>
  <p>vLLM leverages PyTorch as the hardware abstraction layer, enabling deployment across any accelerator.</p>

  <div style="text-align: center; margin: 30px 0;">
    <div class="flow">
      <div class="flow-box">Models</div>
      <div class="flow-box">Utilities</div>
    </div>
    <div style="font-size: 1.4em; color: var(--text-muted); margin: 8px 0;">&darr;</div>
    <div class="flow-box" style="background: linear-gradient(135deg, rgba(238,76,44,0.15), rgba(238,76,44,0.05)); border-color: #EE4C2C; display: inline-block; padding: 16px 50px; font-size: 0.9em;">
      <span style="color: #EE4C2C; font-weight: 800;">PyTorch</span>
    </div>
    <div style="font-size: 1.4em; color: var(--text-muted); margin: 8px 0;">&darr;</div>
    <div class="chip-row" style="justify-content: center;">
      <div class="chip chip-green">NVIDIA GPU (CUDA)</div>
      <div class="chip chip-red">AMD GPU (ROCm)</div>
      <div class="chip chip-blue">Intel GPU (XPU)</div>
      <div class="chip chip-gold">Google TPU</div>
      <div class="chip" style="border-color: rgba(255,255,255,0.2); color: var(--text-muted);">AWS Trainium</div>
    </div>
  </div>

  <div class="highlight-box">
    <p><strong style="color: var(--accent);">Key Insight:</strong> PyTorch serves as a universal abstraction layer. Write model code once and deploy on any hardware backend &mdash; from NVIDIA H100s to Google TPUv5e to AMD MI300X.</p>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 7: RAY SERVE -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Layer 2</span>
  <h2><span class="grad-teal">Ray Serve</span> &mdash; Serving Framework</h2>
  <p>A scalable, framework-agnostic model serving library built on the Ray distributed runtime.</p>

  <div class="grid-2">
    <div>
      <div class="card" style="margin-bottom: 15px;">
        <h4>Model Deployment & Scaling</h4>
        <ul>
          <li>Autoscaling based on request queue depth and latency targets</li>
          <li>Multi-model serving on shared GPU pools</li>
          <li>Rolling updates with zero-downtime deployments</li>
          <li>Dynamic model loading/unloading</li>
        </ul>
      </div>
      <div class="card">
        <h4>Request Routing & Load Balancing</h4>
        <ul>
          <li>Prefix-cache-affinity routing for higher cache hit rates</li>
          <li>Locality-aware scheduling across GPU nodes</li>
          <li>OpenAI-compatible API endpoints out of the box</li>
        </ul>
      </div>
    </div>
    <div>
      <div class="card" style="margin-bottom: 15px;">
        <h4>Model Lifecycle Management</h4>
        <ul>
          <li>Health checks and automatic failure recovery</li>
          <li>Graceful shutdown with request draining</li>
          <li>Model versioning and A/B testing</li>
          <li>Resource quotas per deployment</li>
        </ul>
      </div>
      <div class="card">
        <h4>Advanced Serving Patterns</h4>
        <ul>
          <li><strong style="color: var(--accent2);">Disaggregated Prefill/Decode</strong> &mdash; Separate prefill and decode phases onto dedicated GPU pools</li>
          <li><strong style="color: var(--accent2);">Wide Expert Parallelism</strong> &mdash; For MoE models like DeepSeek</li>
        </ul>
      </div>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 8: KUBERAY -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-purple">Layer 3</span>
  <h2><span class="grad-purple">KubeRay</span> &mdash; Kubernetes Operator</h2>
  <p>KubeRay bridges Ray's distributed compute with Kubernetes-native orchestration.</p>

  <div class="grid-2">
    <div>
      <h3 style="font-size: 0.9em; color: var(--accent3);">Without KubeRay</h3>
      <div class="card">
        <ul>
          <li>Manual Ray head/worker node configuration</li>
          <li>Hand-managed networking between Ray nodes</li>
          <li>No auto-recovery on node failures</li>
          <li>Manual GPU resource allocation</li>
          <li>No integration with K8s RBAC, secrets, or storage</li>
        </ul>
      </div>
    </div>
    <div>
      <h3 style="font-size: 0.9em; color: var(--accent2);">With KubeRay</h3>
      <div class="card" style="border-color: rgba(0,212,170,0.2);">
        <ul>
          <li><strong style="color: var(--accent2);">RayCluster CRD</strong> &mdash; Declarative cluster provisioning</li>
          <li><strong style="color: var(--accent2);">RayService CRD</strong> &mdash; Manages Ray Serve deployments with rolling updates</li>
          <li><strong style="color: var(--accent2);">RayJob CRD</strong> &mdash; Batch jobs with automatic cluster lifecycle</li>
          <li>Auto-healing, GPU-aware scheduling, node pool integration</li>
          <li>Native K8s secrets, ConfigMaps, PVCs, and service mesh support</li>
        </ul>
      </div>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 9: KUBERNETES FOUNDATION -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-purple">Layer 4</span>
  <h2><span class="grad-purple">Kubernetes</span> &mdash; The Foundation</h2>
  <p>The orchestration layer that makes GPU-accelerated LLM serving reliable and scalable.</p>

  <div class="grid-3">
    <div class="card">
      <div class="num num-purple">01</div>
      <h4>GPU Scheduling</h4>
      <p>NVIDIA device plugin, GPU sharing (MIG, time-slicing), topology-aware scheduling for NVLink/NVSwitch</p>
    </div>
    <div class="card">
      <div class="num num-purple">02</div>
      <h4>Node Pool Management</h4>
      <p>Dedicated GPU node pools (L4, A100, H100), spot/preemptible nodes for cost optimization, cluster autoscaler</p>
    </div>
    <div class="card">
      <div class="num num-purple">03</div>
      <h4>Networking</h4>
      <p>RDMA/InfiniBand for multi-node tensor parallelism, service mesh for traffic management, ingress for API endpoints</p>
    </div>
    <div class="card">
      <div class="num num-purple">04</div>
      <h4>Storage</h4>
      <p>Persistent volumes for model weights, HuggingFace cache mounts, shared storage across Ray workers</p>
    </div>
    <div class="card">
      <div class="num num-purple">05</div>
      <h4>Security</h4>
      <p>RBAC, network policies, secrets management for HF tokens and API keys, pod security standards</p>
    </div>
    <div class="card">
      <div class="num num-purple">06</div>
      <h4>Observability</h4>
      <p>Prometheus metrics from vLLM + Ray, Grafana dashboards, GPU utilization monitoring, cost tracking</p>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 10: REQUEST FLOW -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Data Flow</span>
  <h2>End-to-End <span class="grad-teal">Request Flow</span></h2>

  <div class="flow" style="margin: 35px 0;">
    <div class="flow-box" style="border-color: rgba(255,255,255,0.3);">
      Client<br><small style="font-size:0.75em; color: var(--text-muted);">OpenAI SDK</small>
    </div>
    <div class="flow-arrow">&rarr;</div>
    <div class="flow-box" style="border-color: rgba(124,92,252,0.4);">
      K8s Ingress<br><small style="font-size:0.75em; color: var(--text-muted);">Load Balancer</small>
    </div>
    <div class="flow-arrow">&rarr;</div>
    <div class="flow-box" style="border-color: rgba(0,212,170,0.4);">
      Ray Serve<br><small style="font-size:0.75em; color: var(--text-muted);">Router</small>
    </div>
    <div class="flow-arrow">&rarr;</div>
    <div class="flow-box" style="border-color: rgba(255,107,53,0.4);">
      vLLM Engine<br><small style="font-size:0.75em; color: var(--text-muted);">GPU Worker</small>
    </div>
    <div class="flow-arrow">&rarr;</div>
    <div class="flow-box" style="border-color: rgba(255,255,255,0.3);">
      Response<br><small style="font-size:0.75em; color: var(--text-muted);">Streaming</small>
    </div>
  </div>

  <div class="grid-2" style="margin-top: 20px;">
    <div class="card">
      <h4>Prefill Phase</h4>
      <p>Process the entire input prompt in parallel. This is compute-bound and benefits from high FLOPS GPUs. Prefix caching avoids recomputation for shared system prompts.</p>
    </div>
    <div class="card">
      <h4>Decode Phase</h4>
      <p>Generate tokens one at a time autoregressively. This is memory-bandwidth-bound. Continuous batching interleaves multiple requests to keep GPUs saturated.</p>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 11: PERFORMANCE -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-orange">Performance</span>
  <h2><span class="grad-orange">Production</span> Benchmarks</h2>

  <div class="grid-4" style="margin: 30px 0;">
    <div class="card metric">
      <div class="value grad-orange">2.2K</div>
      <div class="label">Tokens/s per H200</div>
    </div>
    <div class="card metric">
      <div class="value grad-teal">24x</div>
      <div class="label">Throughput vs naive</div>
    </div>
    <div class="card metric">
      <div class="value grad-purple">&lt;100ms</div>
      <div class="label">Time to first token</div>
    </div>
    <div class="card metric">
      <div class="value grad-orange">95%+</div>
      <div class="label">GPU Utilization</div>
    </div>
  </div>

  <div class="highlight-box">
    <p><strong style="color: var(--accent);">Key vLLM Optimizations:</strong> PagedAttention reduces memory waste by up to 55%. Continuous batching improves throughput by 2-4x over static batching. Prefix caching accelerates repeated prompts by 3-10x.</p>
  </div>

  <table class="compare-table">
    <thead>
      <tr><th>Feature</th><th>vLLM</th><th>TGI</th><th>Triton + TRT-LLM</th></tr>
    </thead>
    <tbody>
      <tr><td>PagedAttention</td><td style="color: var(--accent2);">&#10003;</td><td style="color: var(--accent2);">&#10003;</td><td style="color: var(--text-muted);">Custom KV Cache</td></tr>
      <tr><td>Continuous Batching</td><td style="color: var(--accent2);">&#10003;</td><td style="color: var(--accent2);">&#10003;</td><td style="color: var(--accent2);">&#10003;</td></tr>
      <tr><td>Multi-Node TP/PP</td><td style="color: var(--accent2);">&#10003;</td><td style="color: var(--text-muted);">Limited</td><td style="color: var(--accent2);">&#10003;</td></tr>
      <tr><td>Open Source</td><td style="color: var(--accent2);">Apache 2.0</td><td style="color: var(--accent2);">Apache 2.0</td><td style="color: var(--text-muted);">Mixed</td></tr>
      <tr><td>KubeRay Native</td><td style="color: var(--accent2);">&#10003;</td><td style="color: var(--text-muted);">&#10007;</td><td style="color: var(--text-muted);">&#10007;</td></tr>
    </tbody>
  </table>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 12: DEPLOYMENT EXAMPLE -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Implementation</span>
  <h2><span class="grad-teal">RayService</span> Deployment</h2>
  <p>Deploy Llama 3.1 8B on GKE with KubeRay in a single manifest.</p>

  <pre><code>apiVersion: ray.io/v1
kind: RayService
metadata:
  name: llama-31-serve
spec:
  serveConfigV2: |
    applications:
      - name: llm
        import_path: ray_serve_llm:app
        deployments:
          - name: VLLMDeployment
            num_replicas: 2
            ray_actor_options:
              num_gpus: 1
  rayClusterConfig:
    headGroupSpec:
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray-ml:2.40.0-gpu
              resources:
                limits:
                  nvidia.com/gpu: "1"
              env:
                - name: HUGGING_FACE_HUB_TOKEN
                  valueFrom:
                    secretKeyRef:
                      name: hf-secret
                      key: token</code></pre>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 13: SCALING PATTERNS -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-purple">Scale</span>
  <h2><span class="grad-purple">Scaling</span> Patterns</h2>

  <div class="grid-2">
    <div>
      <div class="card" style="margin-bottom: 15px;">
        <h4>Single-Node, Multi-GPU</h4>
        <p><strong style="color: var(--accent);">Tensor Parallelism</strong> &mdash; Split model layers across 2-8 GPUs on a single node using NVLink for fast inter-GPU communication. Ideal for models up to ~70B parameters.</p>
        <pre><code style="font-size: 0.85em; padding: 12px 16px;">llm = LLM(
  model="meta-llama/Llama-3.1-70B",
  tensor_parallel_size=4
)</code></pre>
      </div>
    </div>
    <div>
      <div class="card" style="margin-bottom: 15px;">
        <h4>Multi-Node, Multi-GPU</h4>
        <p><strong style="color: var(--accent3);">Pipeline Parallelism</strong> &mdash; Distribute model stages across nodes. KubeRay manages the Ray cluster spanning multiple K8s pods with RDMA networking.</p>
        <pre><code style="font-size: 0.85em; padding: 12px 16px;">llm = LLM(
  model="meta-llama/Llama-3.1-405B",
  tensor_parallel_size=8,
  pipeline_parallel_size=4
)  # 32 GPUs across 4 nodes</code></pre>
      </div>
    </div>
  </div>

  <div class="card" style="margin-top: 15px;">
    <h4>Data Parallel Replicas</h4>
    <p>For throughput scaling, deploy multiple independent model replicas behind Ray Serve's load balancer. Each replica uses TP within its node; Ray Serve routes requests across replicas with prefix-cache affinity.</p>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 14: OBSERVABILITY -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Monitoring</span>
  <h2><span class="grad-teal">Observability</span> Stack</h2>
  <p>Unified metrics pipeline from vLLM engine through Ray to Kubernetes.</p>

  <div class="grid-3">
    <div class="card">
      <h4 style="color: var(--accent);">vLLM Engine Metrics</h4>
      <ul>
        <li>Time to First Token (TTFT)</li>
        <li>Time per Output Token (TPOT)</li>
        <li>Tokens/second throughput</li>
        <li>KV cache utilization %</li>
        <li>Prefix cache hit rate</li>
        <li>Batch size distribution</li>
        <li>Request queue depth</li>
      </ul>
    </div>
    <div class="card">
      <h4 style="color: var(--accent2);">Ray Serve Metrics</h4>
      <ul>
        <li>Replica count &amp; health</li>
        <li>Request latency (p50/p95/p99)</li>
        <li>Queue length per deployment</li>
        <li>Autoscaling events</li>
        <li>Error rates by endpoint</li>
        <li>Active connections</li>
        <li>Routing decisions</li>
      </ul>
    </div>
    <div class="card">
      <h4 style="color: var(--accent3);">Kubernetes Metrics</h4>
      <ul>
        <li>GPU utilization (DCGM)</li>
        <li>GPU memory usage</li>
        <li>Node pool capacity</li>
        <li>Pod restart counts</li>
        <li>Network throughput</li>
        <li>Storage IOPS</li>
        <li>Cost per inference</li>
      </ul>
    </div>
  </div>

  <div class="highlight-box" style="margin-top: 15px;">
    <p><strong>Unified Prometheus Endpoint:</strong> Ray integrates with vLLM to expose all engine-level metrics alongside cluster metrics through a single Prometheus scrape target. Grafana dashboards provide real-time visibility across the full stack.</p>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 15: FUTURE -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-orange">Looking Ahead</span>
  <h2>Future of <span class="grad-orange">LLM Infrastructure</span></h2>

  <div class="grid-2" style="margin-top: 20px;">
    <div>
      <div class="card" style="margin-bottom: 15px;">
        <div class="num num-orange">01</div>
        <h4>Disaggregated Serving</h4>
        <p>Separate prefill and decode into dedicated GPU pools optimized for each phase's compute profile. vLLM + Ray Serve already supports this pattern.</p>
      </div>
      <div class="card">
        <div class="num num-orange">02</div>
        <h4>Specialized Hardware</h4>
        <p>Continued advancements in purpose-built inference chips (NVIDIA Blackwell, Google TPUv6, AMD MI400) drive serving cost down by 2-3x per generation.</p>
      </div>
    </div>
    <div>
      <div class="card" style="margin-bottom: 15px;">
        <div class="num num-teal">03</div>
        <h4>Optimized Serving Engines</h4>
        <p>Speculative decoding, chunked prefill, and structured output guarantees continue to improve vLLM's tokens/s per dollar efficiency.</p>
      </div>
      <div class="card">
        <div class="num num-teal">04</div>
        <h4>Kubernetes-Native LLM Ops</h4>
        <p>Projects like llm-d bring vLLM-native workload orchestration directly into Kubernetes, with inference-aware scheduling and routing.</p>
      </div>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 16: CONCLUSION -->
<!-- ============================================ -->
<section data-background-color="#0a0a0f">
  <span class="tag tag-teal">Conclusion</span>
  <h2>Key <span class="grad-teal">Takeaways</span></h2>

  <div class="grid-2" style="margin-top: 25px;">
    <div>
      <div class="card" style="margin-bottom: 15px; border-left: 3px solid var(--accent);">
        <h4>Scalable Infrastructure</h4>
        <p>Kubernetes and KubeRay enable scalable, adaptable infrastructure to handle the demands of GenAI, simplifying LLM pipelines from development to production.</p>
      </div>
      <div class="card" style="border-left: 3px solid var(--accent2);">
        <h4>Efficient LLM Serving</h4>
        <p>Ray Serve and vLLM provide a streamlined, unified approach to efficiently manage LLM inference with OpenAI-compatible APIs out of the box.</p>
      </div>
    </div>
    <div>
      <div class="card" style="margin-bottom: 15px; border-left: 3px solid var(--accent3);">
        <h4>Future-Ready Architecture</h4>
        <p>Continued advancements in specialized hardware and optimized serving engines like vLLM are critical to improving serving efficiency and reducing cost.</p>
      </div>
      <div class="card" style="border-left: 3px solid #FFD700;">
        <h4>Unified Stack</h4>
        <p>By leveraging Kubernetes, KubeRay, and vLLM together, we tackle current infrastructure challenges and make LLMs accessible for widespread, high-performance use.</p>
      </div>
    </div>
  </div>
  <div class="slide-footer">AutoscaleWorks</div>
</section>

<!-- ============================================ -->
<!-- SLIDE 17: END -->
<!-- ============================================ -->
<section class="title-slide" data-background-color="#0a0a0f">
  <h1 style="font-size: 2.4em;"><span class="grad-orange">Autoscale</span><span class="grad-teal">Works</span></h1>
  <p class="subtitle">Self-Hosted LLM Serving &bull; Agentic RAG Systems &bull; GPU Kubernetes Infrastructure</p>
  <hr class="divider" style="width: 100px; border-color: rgba(255,107,53,0.4);">
  <p class="author" style="margin-top: 20px;">
    <strong style="color: var(--text-primary);">info@autoscaleworks.ai</strong><br>
    autoscaleworks.ai<br><br>
    <span style="font-size: 0.85em; color: rgba(255,255,255,0.3);">Saddle River Consulting LLC</span>
  </p>
</section>

</div>
</div>

<script src="https://cdn.jsdelivr.net/npm/reveal.js@4.6.1/dist/reveal.js"></script>
<script>
  Reveal.initialize({
    hash: true,
    slideNumber: 'c/t',
    width: 1280,
    height: 720,
    margin: 0,
    transition: 'slide',
    transitionSpeed: 'default',
    backgroundTransition: 'fade',
    center: false,
    plugins: []
  });
</script>
</body>
</html>
